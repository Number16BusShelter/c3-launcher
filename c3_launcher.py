#!/usr/bin/env python3
import os
import sys
import time
import argparse
import requests
import threading
import logging
from dotenv import load_dotenv
import json
from datetime import datetime, timedelta

# Load environment variables from .env file
load_dotenv()

# Get API key from environment variables
api_key = os.getenv("C3_API_KEY")

if not api_key:
    print("‚ùå Error: C3_API_KEY not found in .env file")
    print("Please create a .env file with your API key: C3_API_KEY=your_key_here")
    sys.exit(1)

# Get polling interval from environment or use default
WORKLOAD_POLL = int(os.getenv("WORKLOAD_POLL", "30"))  # Default: 30 seconds

# Base API endpoint
base_url = "https://api.comput3.ai/api/v0"

# Headers
headers = {
    "X-C3-API-KEY": api_key,
    "Content-Type": "application/json",
    "Origin": "https://launch.comput3.ai"
}

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Global node tracking
active_nodes = []
node_failures = {}  # Track failure counts per node
node_threads = {}   # Track monitoring threads per node
should_monitor = True
target_node_count = 1  # Default, will be updated from args
node_type_setting = "alternate"  # Default, will be updated from args
keep_running = False  # Default, will be updated from args


def get_running_workloads():
    """Get all currently running workloads"""
    url = f"{base_url}/workloads"
    data = {"running": True}

    response = requests.post(url, headers=headers, json=data)

    if response.status_code == 200:
        return response.json()
    else:
        logger.error(f"‚ùå Error getting workloads: {response.status_code}")
        logger.error(response.text)
        return []


def launch_workload(workload_type="ollama_webui:fast"):
    """Launch a new workload of the specified type"""
    url = f"{base_url}/launch"

    # Always set expiration to current time + 3600 seconds (1 hour)
    current_time = int(time.time())
    expires = current_time + 3600

    # Create launch data
    data = {
        "type": workload_type,
        "expires": expires
    }

    response = requests.post(url, headers=headers, json=data)

    if response.status_code == 200:
        result = response.json()
        result["type"] = workload_type  # Add type to result for easier tracking
        result["expires"] = expires     # Add expiration for tracking
        return result
    else:
        logger.error(f"‚ùå Error launching workload: {response.status_code}")
        logger.error(response.text)
        return None


def stop_workload(workload_id):
    """Stop a running workload by its ID"""
    url = f"{base_url}/stop"

    data = {"workload": workload_id}

    response = requests.post(url, headers=headers, json=data)

    if response.status_code == 200:
        return response.json()
    else:
        logger.error(f"‚ùå Error stopping workload: {response.status_code}")
        logger.error(response.text)
        return None


def stop_all_workloads():
    """Stop all active workloads and log the results"""
    logger.info(f"üõë Stopping all {len(active_nodes)} active workloads...")

    for node_info in active_nodes:
        node_hostname = node_info.get('node')
        node_id = node_info.get('workload')

        logger.info(f"üõë Stopping {node_hostname} (ID: {node_id})...")
        result = stop_workload(node_id)

        if result:
            stopped_time = datetime.fromtimestamp(result.get('stopped')).strftime('%Y-%m-%d %H:%M:%S')
            refund = result.get('refund_amount', 0)
            logger.info(f"‚úÖ Successfully stopped {node_hostname} at {stopped_time} (Refund: {refund})")
        else:
            logger.error(f"‚ùå Failed to stop {node_hostname}")

    logger.info("‚úÖ All workloads stopped")


def check_node_health(node):
    """Check if a node is responding"""
    node_url = f"https://{node}"
    headers = {"X-C3-API-KEY": api_key}

    try:
        response = requests.get(node_url, headers=headers, timeout=5)
        return response.status_code == 200
    except (requests.RequestException, Exception) as e:
        logger.warning(f"üîç Health check failed for {node}: {str(e)}")
        return False


def ensure_target_node_count():
    """Make sure we have the target number of nodes running"""
    global active_nodes

    # Only ensure target count if keep_running is True
    if not keep_running:
        return len(active_nodes)

    current_count = len(active_nodes)

    if current_count < target_node_count:
        nodes_to_add = target_node_count - current_count
        logger.info(f"üîÑ Current node count ({current_count}) is below target ({target_node_count}). Launching {nodes_to_add} additional nodes...")

        for i in range(nodes_to_add):
            # Determine node type based on parameter
            if node_type_setting == "alternate":
                # Calculate the index as if we're continuing the sequence
                idx = current_count + i
                workload_type = "ollama_webui:fast" if idx % 2 == 0 else "ollama_webui:large"
            else:
                workload_type = f"ollama_webui:{node_type_setting}"

            logger.info(f"üîÑ Launching replacement node {i+1}/{nodes_to_add} ({workload_type})...")
            result = launch_workload(workload_type=workload_type)

            if result:
                active_nodes.append(result)
                logger.info(f"‚úÖ Successfully launched node: {result.get('node')} (ID: {result.get('workload')})")

                # Initialize failure counter
                node_failures[result.get('node')] = 0

                # Start monitoring the new node
                start_node_monitoring(result)

                # Small delay to avoid rate limiting
                time.sleep(2)
            else:
                logger.error(f"‚ùå Failed to launch replacement node")

    return len(active_nodes)


def remove_failed_node(node_info):
    """Remove a failed node from tracking and stop it if possible"""
    global active_nodes

    node_hostname = node_info.get('node')
    node_id = node_info.get('workload')

    logger.info(f"üîÑ Removing failed node {node_hostname} (ID: {node_id})")

    # Stop the failed node
    stop_result = stop_workload(node_id)
    if stop_result:
        logger.info(f"‚úÖ Successfully stopped failed node {node_hostname}")
    else:
        logger.warning(f"‚ö†Ô∏è Failed to stop node {node_hostname}, but continuing removal")

    # Remove from active nodes list
    active_nodes = [node for node in active_nodes if node.get('workload') != node_id]

    # Ensure we maintain the target node count if keep_running is enabled
    if keep_running:
        ensure_target_node_count()


def monitor_node(node_info):
    """Monitor a single node in a dedicated thread"""
    global active_nodes

    node_hostname = node_info.get('node')
    node_id = node_info.get('workload')
    node_type = node_info.get('type', 'unknown')

    logger.info(f"üîé Starting monitoring thread for node {node_hostname} ({node_type})")

    # Initial status check
    is_healthy = check_node_health(node_hostname)
    if is_healthy:
        logger.info(f"‚úÖ Node {node_hostname} is up and running")
    else:
        logger.warning(f"‚ö†Ô∏è Initial health check failed for node {node_hostname}, will retry")

    while should_monitor and node_info in active_nodes:
        # Check if the node is still in workloads response
        current_workloads = get_running_workloads()
        workload_ids = [w.get('workload') for w in current_workloads]

        if node_id not in workload_ids:
            logger.warning(f"‚ö†Ô∏è Node {node_hostname} (ID: {node_id}) is no longer in workloads response, considering it removed")
            # Remove from active nodes
            active_nodes = [node for node in active_nodes if node.get('workload') != node_id]
            # Ensure target count if keep_running is enabled
            if keep_running:
                ensure_target_node_count()
            break

        # Node is still in workloads, check health with retries
        is_alive = False
        for retry in range(3):  # Try up to 3 times (initial + 2 retries)
            is_healthy = check_node_health(node_hostname)
            if is_healthy:
                is_alive = True
                break
            else:
                logger.warning(f"‚ö†Ô∏è Health check failed for {node_hostname} (attempt {retry+1}/3)")

        if not is_alive:
            logger.error(f"‚ùå Node {node_hostname} failed all health checks, removing...")
            remove_failed_node(node_info)
            break
        else:
            logger.info(f"‚úÖ Node {node_hostname} is healthy")

        # Sleep before next check
        time.sleep(WORKLOAD_POLL)

    logger.info(f"üõë Monitoring stopped for node {node_hostname}")


def start_node_monitoring(node_info):
    """Start a dedicated monitoring thread for a node"""
    node_hostname = node_info.get('node')

    # Create and start a monitoring thread for this node
    thread = threading.Thread(
        target=monitor_node,
        args=(node_info,),
        daemon=True,
        name=f"monitor-{node_hostname}"
    )
    thread.start()

    # Store the thread reference
    node_threads[node_hostname] = thread

    logger.info(f"üîç Monitoring thread started for {node_hostname} (polling every {WORKLOAD_POLL} seconds)")
    return thread


def launch_nodes(num_nodes=1, keep_nodes_running=False, node_type="alternate"):
    """Launch the specified number of nodes"""
    global active_nodes, target_node_count, node_type_setting, keep_running

    # Update global settings
    target_node_count = num_nodes
    node_type_setting = node_type
    keep_running = keep_nodes_running

    logger.info(f"üöÄ Launching {num_nodes} nodes (keep running: {keep_running}, type: {node_type})")
    logger.info(f"üìä Node health polling interval: {WORKLOAD_POLL} seconds")

    # Calculate expiration time (even if keep_running is True, we set expires to current + 3600)
    current_time = int(time.time())

    successful_launches = []

    for i in range(num_nodes):
        # Determine node type based on parameter
        if node_type == "alternate":
            workload_type = "ollama_webui:fast" if i % 2 == 0 else "ollama_webui:large"
        else:
            workload_type = f"ollama_webui:{node_type}"

        logger.info(f"üîÑ Launching node {i+1}/{num_nodes} ({workload_type})...")
        result = launch_workload(workload_type=workload_type)

        if result:
            successful_launches.append(result)
            logger.info(f"‚úÖ Node {i+1}: {result.get('node')} (ID: {result.get('workload')})")

            # Small delay to allow the node to boot up
            logger.info(f"‚è≥ Waiting 5 seconds for node to initialize...")
            time.sleep(5)

            # Initialize failure counter
            node_failures[result.get('node')] = 0
        else:
            logger.error(f"‚ùå Failed to launch node {i+1}")

    # Print summary
    logger.info("\n=== üìä Launch Summary ===")
    logger.info(f"Requested: {num_nodes} nodes")
    logger.info(f"Successful: {len(successful_launches)} nodes")

    for idx, node in enumerate(successful_launches):
        node_type_str = node.get("type", "unknown")

        expiry_time = datetime.fromtimestamp(node.get('expires')).strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"{idx+1}. {node.get('node')} (Type: {node_type_str}, Expires: {expiry_time})")

    # Update active nodes
    active_nodes = successful_launches

    # Start monitoring threads for each node
    for node_info in active_nodes:
        start_node_monitoring(node_info)

    # If we couldn't launch all requested nodes and keep_running is enabled, try to reach the target count
    if keep_running and len(successful_launches) < num_nodes:
        ensure_target_node_count()


def main():
    parser = argparse.ArgumentParser(description="Comput3.ai Workload Manager")
    parser.add_argument("--nodes", type=int, default=1, help="Number of nodes to launch (default: 1)")
    parser.add_argument("--runtime", type=int, help="Runtime in seconds (default: 3600)")
    parser.add_argument("--keep-running", action="store_true", help="Keep relaunching nodes when they fail or expire")
    parser.add_argument("--poll", type=int, help="Node health check interval in seconds (default: 30)")
    parser.add_argument("--type", type=str, default="alternate", choices=["fast", "large", "alternate"],
                      help="Node type to launch (fast, large, or alternate) (default: alternate)")
    parser.add_argument("--no-rm", action="store_true", help="Keep nodes running after script terminates")

    args = parser.parse_args()

    # Validate arguments
    if args.nodes < 1:
        logger.error("‚ùå Error: Number of nodes must be at least 1")
        sys.exit(1)

    # Update polling interval if specified
    if args.poll is not None and args.poll > 0:
        global WORKLOAD_POLL
        WORKLOAD_POLL = args.poll
        logger.info(f"üìä Setting node health polling interval to {WORKLOAD_POLL} seconds")

    # Launch nodes with specified parameters
    launch_nodes(args.nodes, args.keep_running, node_type=args.type)

    try:
        # Keep the script running to allow monitoring
        while any(thread.is_alive() for thread in node_threads.values()):
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("üëã Shutting down monitoring...")
        global should_monitor
        should_monitor = False

        # Wait for monitoring threads to finish
        for hostname, thread in node_threads.items():
            logger.info(f"Waiting for {hostname} monitoring to complete...")
            thread.join(timeout=2)

        # Stop all workloads unless --no-rm flag is specified
        if not args.no_rm:
            logger.info("Stopping all workloads before exit (use --no-rm to keep them running)")
            stop_all_workloads()
        else:
            logger.info("Leaving all workloads running (--no-rm flag is set)")

        logger.info("Bye! üëã")


if __name__ == "__main__":
    main()
